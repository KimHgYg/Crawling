{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "stopwords = ['.',',','\\n','\\xa0',re.compile('^A-Za-z*$')]\n",
    "vec = TfidfVectorizer(stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import sshtunnel\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "def init_mysql():\n",
    "    global cnx\n",
    "    cnx = mysql.connector.connect(user='user1', password='tkfkdgody1!', host='52.79.185.101', database='Crawling')\n",
    "    global cursor\n",
    "    cursor = cnx.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_foreground = (\"INSERT IGNORE INTO foreground (title, press, link, day, class) \"\n",
    "                  \"VALUES (%(title)s, %(press)s, %(link)s, %(day)s, %(class)s)\")\n",
    "add_background = (\"INSERT IGNORE INTO background (tfidf,link) \"\n",
    "                 \"VALUES (%(tfidf)s, %(link)s)\")\n",
    "clear_db = (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "def get_html(url):\n",
    "    _html = \"\"\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding='utf-8'\n",
    "    if resp.status_code == 200 :\n",
    "        _html = resp.text\n",
    "    return _html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정치 경제 스포츠 연예 사회 국제\n",
    "#politics economy sports enter national inter\n",
    "#조선일보 경제는 biz.chosun.com...\n",
    "CHOSUN = [(\"http://news.chosun.com/politics\",'politics'),(\"http://news.chosun.com/sports\",'sports'), (\"http://news.chosun.com/ent\",'enter'), (\"http://news.chosun.com/national\",'national'),( \"http://news.chosun.com/international\",'inter')]\n",
    "\n",
    "#동아일보는 패턴 통일\n",
    "DONGA = [(\"http://news.donga.com/Politics\",'politics'),(\"http://news.donga.com/Economy\",'economy'),(\"http://news.donga.com/Sports\",'sports'),(\"http://news.donga.com/Enter\",'enter'),(\"http://news.donga.com/Society\",'national'),(\"http://news.donga.com/Inter\",'inter')]\n",
    "\n",
    "#중앙일보 연예가 가요, 방송, 영화로 나눠져있음\n",
    "JOONGANG = [(\"http://news.joins.com/politics\",'politics'),(\"http://news.joins.com/money\",'economy'),(\"http://news.joins.com/sports\",'sports'),(\"http://news.joins.com/society\",'national'),(\"http://news.joins.com/world\",'inter')]\n",
    "JOONGANG_CUL = [(\"http://news.joins.com/culture/song/list\",'enter'),(\"http://news.joins.com/culture/broadcast/list\",'enter'),(\"http://news.joins.com/culture/movie/list\",'enter')]\n",
    "\n",
    "#기사 내용을 담을 변수\n",
    "\n",
    "num_press = 3\n",
    "num_class = 6\n",
    "docs = [[] for i in range(num_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    중앙일보 날짜 가져오기 힘들어서 datetime 패키지를 이용하여 임의로 날짜 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chosun_crawling():\n",
    "    data_foreground = []\n",
    "    i = 0\n",
    "    for clas in CHOSUN:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        for lt in soup.find_all(\"dt\") :\n",
    "            try :\n",
    "                tmp = lt.find('a')\n",
    "                link = tmp.get('href')\n",
    "                day = link.split('/')\n",
    "                day = day[6]+day[7]+day[8]\n",
    "                text = tmp.get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day, 'class' : clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class' : 'par'}).get_text(),link))\n",
    "            except : \n",
    "                print('error!!!',link)\n",
    "        #docs[i].append(tmp_list)\n",
    "        i = i + 1\n",
    "        if i == 1 :\n",
    "            i = i + 1\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return\n",
    "\n",
    "def chosun_biz():\n",
    "    html = get_html('http://biz.chosun.com')\n",
    "    print('http://biz.chosun.com')\n",
    "    soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "    data_foreground = []\n",
    "    head = soup.find(\"div\", attrs={'class' : 'mt_art_tit'})\n",
    "    tmp = head.find('a')\n",
    "    link = tmp.get('href')\n",
    "    day = link.split('/')\n",
    "    day = day[6]+day[7]+day[8]\n",
    "    text = tmp.get_text()\n",
    "    data_foreground.append({'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day, 'class' : 'economy'})\n",
    "    \n",
    "    html = get_html(link)\n",
    "    soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "    docs[1].append((soup2.find('div',id = 'article_2011', attrs={'class':'article'}).get_text(),link))\n",
    "    \n",
    "    art = soup.find(\"div\", attrs={'class' : 'mc_art_lst'})\n",
    "    for lt in art.find_all(\"li\", limit = 30) :\n",
    "        try :\n",
    "            tmp = lt.find('a')\n",
    "            #print(tmp)\n",
    "            link = tmp.get('href')\n",
    "            day = link.split('/')\n",
    "            day = day[6]+day[7]+day[8]\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day,'class':'economy'})\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[1].append((soup2.find('div',id = 'article_2011', attrs={'class':'article'}).get_text(),link))\n",
    "        except : \n",
    "            print('error!!!',link)\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donga_crawling():\n",
    "    data_foreground = []\n",
    "    #docs = []\n",
    "    i = 0\n",
    "    for clas in DONGA:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        try:\n",
    "            tmp = soup.find('div',attrs={'class':'articleTop'})\n",
    "            head = tmp.find(\"div\", attrs={'class' : 'articleMain'})\n",
    "            #해드라인_main\n",
    "            head_main = head.find('a')\n",
    "            link = head_main.get('href')\n",
    "            day = link.split('/')[6]\n",
    "            text = head.find(attrs={'class' : 'title'}).get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            #print(data_foreground)\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "            for head_sub in tmp.find_all('li') :\n",
    "                art = head_sub.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "\n",
    "        except :\n",
    "            for head in tmp.find_all('div',attrs={'class':'artivleMain'}):\n",
    "                art = head.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "\n",
    "                \n",
    "            for head in tmp.find_all('div',attrs={'class':'artivleMain02'}):\n",
    "                art = head.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "\n",
    "        try:\n",
    "            #issue\n",
    "            content_issue = soup.find('div',attrs={'class':'issueList'})\n",
    "            issue = content_issue.find('a', attrs={'class' : 'tit'})\n",
    "            link = issue.get('href')\n",
    "            day = link.split('/')[6]\n",
    "            text = issue.get_text();\n",
    "            data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div',attrs={'class':'atrticle_txt'}).get_text(),link))\n",
    "            for issue_sub in content_issue.find_all('li') :\n",
    "                art = issue_sub.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "        except :\n",
    "            print('error!!!',link)\n",
    "            \n",
    "        try:\n",
    "            #최신기사\n",
    "            contents = soup.find('div',attrs={'class':'articleList_con'})\n",
    "            for art in contents.find_all('div',attrs={'class':'rightList'}) :\n",
    "                tmp = art.find('a')\n",
    "                link = tmp.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = tmp.find(attrs={'class':'tit'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "        except :\n",
    "            print('error!!!',link)\n",
    "        i = i + 1\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joongang_crawling():\n",
    "    day = datetime.today().strftime(\"%Y%m%d\")\n",
    "    data_foreground = []\n",
    "    i = 0\n",
    "    for clas in JOONGANG:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        origin = soup.find('div',id='content')\n",
    "        #해드라인\n",
    "        try:\n",
    "            art = origin.find('dt')\n",
    "            tmp = art.find('a')\n",
    "            link = tmp.get('href')\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html2 = get_html(link)\n",
    "            soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "        except :\n",
    "            #sport 해드라인\n",
    "            try:\n",
    "                for art in origin.find_all('div',attrs={'class':'slide'}):\n",
    "                    tmp = art.find(attrs={'class':'headline mg'})\n",
    "                    tmp2 = tmp.find('a')\n",
    "                    link = tmp2.get('href')\n",
    "                    text = tmp2.get_text();\n",
    "                    data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                    html2 = get_html(link)\n",
    "                    soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                    docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "            except:\n",
    "                print('error!!!',link)\n",
    "        \n",
    "        #실시간 주요뉴스\n",
    "        try:\n",
    "            art = origin.find('div',attrs={'class':'default_realtime'})\n",
    "            art = art.find('ul',id='ulItems')\n",
    "            #print(art)\n",
    "            for content in art.find_all('li'):\n",
    "                #print(content)\n",
    "                tmp = content.find(attrs={'class':'headline mg'})\n",
    "                tmp2 = tmp.find('a')\n",
    "                link = 'http://news.joins.com/' + tmp2.get('href')\n",
    "                text = tmp2.get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html2 = get_html(link)\n",
    "                soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "        except:\n",
    "            try:\n",
    "                art = origin.find('div',attrs={'class':'combination_today'})\n",
    "                art = art.find('div',attrs={'class':'bd'})\n",
    "                for content in art.find_all('li'):\n",
    "                    tmp = content.find(attrs={'class':'headline mg'})\n",
    "                    tmp2 = tmp.find('a')\n",
    "                    link = 'http://news.joins.com/' + tmp2.get('href')\n",
    "                    text = tmp2.get_text()\n",
    "                    data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                    html2 = get_html(link)\n",
    "                    soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                    docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "            except :\n",
    "                print('error!!!',link)\n",
    "        i = i + 1\n",
    "        if i == 3 :\n",
    "            i = i + 1\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return \n",
    "\n",
    "def joongang_cul():\n",
    "    day = datetime.today().strftime(\"%Y%m%d\")\n",
    "    dosc = []\n",
    "    #tmp_list = []\n",
    "    data_foreground = []\n",
    "    for clas in JOONGANG_CUL:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        \n",
    "        origin = soup.find('div',id='content')\n",
    "        for content in origin.find_all('li'):\n",
    "            art = content.find(attrs={'class':'headline mg'})\n",
    "            tmp = art.find('a')\n",
    "            link = 'http://news.joins.com' + tmp.get('href')\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html2 = get_html(link)\n",
    "            soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "            docs[3].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    scikit-Learn 패키지를 이용한 TF-IDF 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, operator\n",
    "import pandas as pd\n",
    "from konlpy.tag import Twitter, Kkma\n",
    "\n",
    "def cal_weight():\n",
    "    tfidf_list = []\n",
    "    tfidf_list_raw = []\n",
    "    t = Twitter()\n",
    "    k = Kkma()\n",
    "    for i in range(num_class):\n",
    "    #for i in range(1):\n",
    "        nouns = []\n",
    "        for article in docs[i]:\n",
    "            if article[0] is not '':\n",
    "                nouns.append(' '.join([noun for noun in t.nouns(str(article[0]))]))        \n",
    "        vec.\n",
    "        fitted = vec.fit(nouns)\n",
    "        tfidf_res = fitted.transform(nouns)\n",
    "        tfidf_pro = np.argmax(tfidf_res.toarray(),axis = 2)\n",
    "        docs_pro = \n",
    "        docs_pro = fitted.transform(nouns)\n",
    "        vocab = fitted.get_feature_names()\n",
    "        j = 0\n",
    "        for article in tfidf_res.toarray():\n",
    "            idf = sorted(zip(vocab,article), key=lambda kv:kv[1])[-3:]\n",
    "            tmp = idf[0][0]+' '+idf[1][0] + ' ' + idf[2][0]\n",
    "            tfidf_list_raw.append(tmp)\n",
    "            tfidf_list.append({'tfidf':tmp,'link':docs[i][j][1]})\n",
    "            j = j + 1\n",
    "    cursor.executemany(add_background, tfidf_list)\n",
    "    cnx.commit()\n",
    "    cursor.execute('delete from foreground where link not in (select link from background);')\n",
    "    cnx.commit()\n",
    "    print('cal_weight done!')\n",
    "    return tfidf_list_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "[['나', '형용'], ['너', '지성'], ['우리', '인간']]\n",
      "['우리', '인간', '지성', '형용']\n",
      "[[0.         0.         0.         1.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.70710678 0.70710678 0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "tw = Twitter()\n",
    "km = Kkma()\n",
    "vector = TfidfVectorizer(analyzer='word')\n",
    "l1 = ['나는 형용','너는 지성','우리는 인간']\n",
    "test = []\n",
    "test_pro = []\n",
    "for article in l1:\n",
    "    test.append(' '.join([noun for noun in tw.nouns(str(article))]))\n",
    "\n",
    "r = vector.fit_transform(test).toarray()\n",
    "voc = vector.get_feature_names()\n",
    "\n",
    "for i in range(len(test)):\n",
    "    test[i] = test[i].split(' ')\n",
    "for text in test:\n",
    "    for word in text:\n",
    "        try:\n",
    "            voc.index(word)\n",
    "        except:\n",
    "            pass\n",
    "print(test)\n",
    "print(voc)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    TF-IDF 값 Word Embedding  :::: 사용자에게 기사 추천\n",
    "    cal_weight() 에 들어가서 단어들 vector로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from future.utils import iteritems\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import *\n",
    "from sklearn.preprocessing import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "import numpy as np\n",
    "\n",
    "def data_processing(vec, docs, tfidf_pro):\n",
    "    #Tokenizer\n",
    "    token = Tokenizer(lower = False, filters = '\\n\\xa0')\n",
    "    #데이터 정제\n",
    "    tfidf_pro = [[] for i in range(len(tfidf_raw))]\n",
    "    j=0\n",
    "    for i in tfidf_raw:\n",
    "        tmp = i.split(' ')\n",
    "        tfidf_pro[j] = tmp\n",
    "        j = j + 1\n",
    "    tfidf_pro = np.array(tfidf_pro)\n",
    "    docs = np.array(docs)\n",
    "    docs_pro = docs[:,0]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pro = [[] for i in range(len(tfidf_raw))]\n",
    "j=0\n",
    "for i in tfidf_raw:\n",
    "    tmp = i.split(' ')\n",
    "    tfidf_pro[j] = tmp\n",
    "    j = j + 1\n",
    "tfidf_pro = np.array(tfidf_pro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\xa0━\\xa0  [최상연 논설위원이 간다]\\xa0 “희생 없는 일신 불가능” vs “물갈이는 결국 못할 것” \\xa0   자유한국당 비대위가 전국 253개 당협위원장 교체 작업에 속도를 올리고 있다. 사무총장 등 당연직 3명이 포함된 조직강화특위는 전원책 변호사가 사실상 위원장 역할을 하는 인선이 마무리 단계다. ‘웰빙 화초를 배제하고 비바람 맞으며 자란 꽃을 찾겠다’는 게 전 변호사의 취임 일성인데 기대와 우려가 반반이다. 개혁의 알파이자 오메가인 인적 청산이 시작된 것이지만 동시에 고질적 계파 갈등의 뇌관을 건드리는 것이어서 한국당은 드디어 ‘진실의 시간’과 마주했다. 인적 쇄신없이 당을 일신하는 건 불가능하지만 폭이 클수록 후폭풍이 거셀 건 불을 보듯 뻔한 일이다.     \\xa0     휴일이었던 9일 서울 영등포동 자유한국당 당사와 여의도 국회의사당 내 한국당 당직자실은 모두 텅 비어 있었다. 당초 전원책 변호사가 조강특위 외부위원 명단을 발표하고 앞으로 활동 방향을 밝힐 예정이었지만 기자회견은 일단 11일로 연기됐다. 몇 차례 연기된 인선이 이날도 막판까지 난항을 거듭했기 때문이다.     \\xa0     외부위원 명단이 나돈 건 지난 주말부터다. 김용태 사무총장 등 내부위원 3명이 의결권을 갖지 않기로 함에 따라 전 변호사를 포함한 이들 외부위원 4명이 실제 칼잡이들이다. 하지만 쉽지 않을 도부수(刀斧手) 역할에 손사래 치는 사람들이 많아 조강특위 출범은 매듭을 짓지 못했다.     \\xa0     칼자루를 쥔 전 변호사는 연일 ‘아무도 희생하지 않는 당의 일신(一新)이란 불가능하다’며 굳은 표정이지만 첫발도 떼기 전 뭔가 걸음이 꼬이는 모양새다. 더구나 그의 다짐은 당내서 긴장감과 함께 역풍도 만들고 있다. 박근혜계는 ‘청산=분열’, 홍준표계는 집단행동을 들고 나왔다.     \\xa0     \\xa0     \\xa0   “현역 의원 손댈 방법 현실적으로 없다”    \\xa0        자유한국당 자체 여론조사 해보니   한국당 초·재선 잔류파 의원을 중심으로 한 ‘통합과 전진’은 지난주 ‘인적 쇄신이 당 분열을 초래할 수 있다’는 우려감을 문서로 담아 김병준 비대위원장에게 전달했다. ‘특정인에 의한 인치적 개혁과 제왕적 개혁을 반대하며 당헌·당규 개정을 통한 시스템 개혁이 돼야 한다’는 보도자료도 냈다.     \\xa0     모임에 참석했던 정용기 의원에게 왜 그런 결론을 냈는지 물었다.     \\xa0    지금 비대위가 인치적 개혁을 하고 있나.“‘인적 청산은 할 수 없고, 할 수 없기 때문에 하지 않는다’는 게 김병준 비대위원장의 말이었다. 갑자기 인적 쇄신을 들고 나왔는데 결정 과정이 민주적이지 않고 절차적 투명성도 없다. 게다가 한국당 안엔 별 사람이 없고, 다 쓸데없는 사람이고, 밖에 있는 ‘들꽃 같은 사람’을 운운하는데 한국당이 구제 불능 환자인 것처럼 인식될 우려가 있다. 언행에 신중해야 한다.”   \\xa0    그럼 비대위 역할은 뭔가.“공정한 게임이 되도록 다음 전당대회 게임의 룰을 만들고 관리자 역할을 하는 게 본질적 역할이다. 가치 중심의 시스템 정당으로 가야 하지만 그건 선출된 지도부가 할 일이다. 사람이 아파서 살려 달라고 의사를 불렀는데 살리는 게 아니라 메스를 갖고 죽어 마땅한 환자라고 비쳐지게 언행을 하면 한국당을 자해하는 결과가 될 수도 있다.”   \\xa0    인적 청산에 반대하나.“거부하는 게 아니다. 다만 하려면 당내 분파가 없어지도록 해야 한다. 보수 통합이 아니라 당내 화합이 먼저다.”   \\xa0     \\xa0   ＂ 당 망가뜨린 건 홍준표·김무성 아닌가”    \\xa0     ‘통합과 전진’ 간사 엄용수 의원에게 향후 계획을 물었더니 “현역 의원을 지금 어떻게 전반적으로 손댈 수 있겠습니까?”란 반문이 돌아왔다. 보수대통합이든 인적 쇄신이든 현실적으로나 시기적으로 어려워 특별히 견제할 것도 없다는 것이었다. 다만 당 대표 출마자의 자격을 제한해 인적 쇄신하자고 주장했다. 탈당 후 해당 행위를 하고 다시 복당한 자, 지방선거 등 각종 선거결과에 책임이 있는 자 등은 출마할 수 없도록 하자는 요구다. 김무성·홍준표 전 대표가 여기에 걸린다.     \\xa0     한때 친박 핵심으로 통했던 김태흠 의원 생각도 비슷했다.     \\xa0    친박이 대상이면 집단행동으로 가나.“친박은 궤멸됐다. 서청원·최경환 의원이 물러난 마당에 누구에게 무슨 책임을 물을 수 있나.”   \\xa0    윤상현 의원 등이 거론되던데.“설사 국민 밉상이라고 치자. 그게 책임을 물을 일인가. 당을 망가뜨린 건 홍준표·김무성 전 대표다. 두 사람을 쳐내지도 못하면서 당협위원장 몇 명 날리는 걸 누가 인적 쇄신이라고 믿어 주겠나.”   \\xa0    인적 청산 대상이란 말을 들은 적 있나.“없다. 나는 당직 맡은 적도 장관 한 적도 없다. 당시 수뇌부가 책임져야 하고 4, 5선 중진 의원들의 불출마가 이어져야 한다.”   \\xa0    실제로 인적 청산이 시작되면 한국당은 분당으로 갈까.“분당이 쉽지도 않을 거다. 실제로 친박이 없으니 친박 청산은 없고, 세 결집도 없다. 그렇다고 수술 방법도 모르는 지금 비대위가 개혁에 성공할 것 같지도 않다. 당이 흙탕물인데 외부인사 수혈이 되겠나.”   \\xa0     여기에다 홍준표 대표 시절 새로 임명된 60여 명의 당협위원장들은 “집단행동 불사”를 외치는 상황이다. 당내엔 ‘어차피 내년 2월 전당대회서 새 대표가 뽑히면 말짱 도루묵이 될 쇄신책’이란 냉소가 확산돼 있다. 하지만 한국당이 무기력과 침체의 늪에서 벗어나려면 인적 쇄신 외엔 답이 없다고 당 구성원들이 느끼는 것도 사실이다.     \\xa0     \\xa0   의원 절반이 “세대교체·인재양성 절실”    \\xa0     당협위원장 인선의 전권을 위임받았다는 전원책 변호사는 ‘욕 먹어도 칼자루를 쓰겠다’고 연일 강도 높은 인적 쇄신을 예고 중이다. 도대체 누구를 언제 얼마나 갈아 치우겠다는 것일까.     \\xa0     전원책 변호사에게 물었다.     \\xa0    홍준표·김무성 두 전 대표의 당권 도전 여부도 인적 쇄신에 올리나.“조강특위는 당협위원장 인선을 위한 기구다. 당 대표 출마 자격을 따지는 건 권한 밖의 일이다.”   \\xa0    두 사람을 포함해 모든 당협위원장이 사퇴했다. 새로 임명하는 과정에서 물갈이한다는 게 조강특위의 인적 쇄신 아닌가.“그것과 관계없이 전당대회엔 출마할 수 있다. 대선주자급으로 논의되는 분들은 당의 중요한 자산인데 함부로 칼을 들이대선 안 된다. 다만 개인적으론 대선 후보였거나 그런 반열에 올랐던 분들이 계파 보스처럼 권력을 다투는 모습을 보이는 건 옳지 않다고 생각한다.”   \\xa0    당협위원장에서 배제해도 의원직을 박탈할 방법은 없다. 물갈이가 탈당 의원만 늘려 당 전력 약화로 이어질 수도 있지 않나.“70~80명의 단단한 의원만으로 당을 꾸려갈 수 있는 상황이 아닌 건 맞다. 개헌, 선거법 외에 당장 판문점선언 비준동의안을 포함해 물러설 수 없는 현안이 많다. 게다가 정기국회 시즌이다. 인적 쇄신이 야당 역할에 방해되거나 분열로 간다면 실패다. 그 반대로 간다.”   \\xa0    어떻게 가능한가.“꼭 칼을 대는 것뿐만 아니라 스스로 명예롭게 물러나거나 책임지도록 유도하고 새 인물 수혈에 비중을 두겠다. 그래도 벌써부터 조기 전당대회 요구가 빗발친다. 양쪽 귀 막고 간다. 욕먹을 각오는 돼 있다.”   \\xa0    쇄신 명단은 언제 나오나.“칼은 허공이든 사람이든 베고 나면 효과가 반감된다. 그렇다고 너무 늦게까지 차고만 있을 수도 없다. 총의를 모아 동의받고 압박하는 식으로 갈 거다. 그리 오래 걸리지 않는다.”   \\xa0    비대위 살생부는 이미 마련됐다던데.“있는지 없는지 모르겠지만 거기엔 따르지 않는다. 내가 완전한 권한을 요구했고 공감대가 있다.”   \\xa0    몇 명이나 손을 대나.“생각은 있는데 아직 특위 위원들과 논의를 못 했다. 중요한 건 과거 기준과 방식에서 벗어날 거란 점이다. 영남에서 지역구 관리를 잘한다는 게 어떤 의미가 있겠나. 의원으로서의 자질이 더 중요한 잣대다.”   \\xa0     \\xa0   “홍준표·김무성 전대 출마 못 막는다”    \\xa0     1시간 가까운 인터뷰에서 그는 ‘강도 높은 인적 청산’을 강도 높게 역설했지만 동시에 ‘계파 싸움에 휘말리면 당을 살리려는 일이 오히려 당 와해와 보수 궤멸로 갈 수 있다’는 신중함도 곁들였다. 바른미래당과의 통합 전대 등 ‘보수 단일 대오’를 위해 노력하겠다는 다짐이 많았다. 그래서 ‘통합전당대회가 정말로 가능하다고 보는지’ 물었더니 ‘그래야 한다’는 원칙론이었다.     \\xa0     “조강특위 위원이 전당대회에 대해 얘기할 권한은 없다. 다만 크게 멀리 보고 가자는 원칙론이다. 강이 바다로 갈 때 합쳐서 흘러내려 간다. 거꾸로 가는 강은 없다.”     \\xa0     전 변호사 뒤엔 김용태 사무총장이 숨어 있다고들 한다. 김 총장은 ‘인적 청산하는 순간 당이 깨질 것’이라고 후유증을 염려하는 쪽이었다. 물갈이 수단이 마땅치 않은 만큼 ‘보수 빅텐트’를 펴는 쪽에 힘을 쏟아야 한다는 주장이었다. 그런데 한국당은 왜 갑자기 인적 청산 분위기 일까.     \\xa0     김 총장에게 질문했다.     \\xa0    당협위원장 교체를 서두르는 배경이 뭔가.“일정이 바뀐 건 없다. 다만 일을 맡게 된 전 변호사가 초기에 이목을 끌어 일종의 긴장감을 만들어야겠다고 판단하는 모양이다.”   \\xa0    김 총장의 살생부 명단이 있다던데.“사실이 아니다. 당협위원장 교체는 정량평가와 정성평가를 종합하는데 정량평가는 늘 하던 방식 그대로 진행 중이다. 문제는 정성평가다.”   \\xa0    홍·김 두 전 대표는 어떻게 처리하나.“대표직 출마를 우리가 막을 길은 없다.”   \\xa0     최상연 논설위원\\xa0   \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    init_mysql()\n",
    "    \n",
    "    joongang_crawling()\n",
    "    joongang_cul()\n",
    "    chosun_crawling()\n",
    "    chosun_biz()\n",
    "    donga_crawling()\n",
    "    \n",
    "    ret = cal_weight()\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.joins.com/politics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:146: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.joins.com/money\n",
      "http://news.joins.com/sports\n",
      "http://news.joins.com/society\n",
      "http://news.joins.com/world\n",
      "http://news.joins.com/culture/song/list\n",
      "http://news.joins.com/culture/broadcast/list\n",
      "http://news.joins.com/culture/movie/list\n",
      "http://news.chosun.com/politics\n",
      "error!!! http://forum.chosun.com/message/messageView.forum?bbs_id=1010&message_id=1382475\n",
      "error!!! http://forum.chosun.com/message/messageView.forum?bbs_id=1010&message_id=1382493\n",
      "http://news.chosun.com/sports\n",
      "http://news.chosun.com/ent\n",
      "http://news.chosun.com/national\n",
      "error!!! http://news.chosun.com/site/data/html_dir/2018/10/10/2018101000210.html\n",
      "error!!! http://news.chosun.com/site/data/html_dir/2018/10/10/2018101000210.html\n",
      "error!!! http://news.chosun.com/site/data/html_dir/2018/10/10/2018101000210.html\n",
      "error!!! http://news.chosun.com/site/data/html_dir/2018/10/10/2018101000210.html\n",
      "error!!! http://forum.chosun.com/message/messageView.forum?bbs_id=1040&message_id=1382401\n",
      "error!!! http://forum.chosun.com/message/messageView.forum?bbs_id=1040&message_id=1382441\n",
      "http://news.chosun.com/international\n",
      "http://biz.chosun.com\n",
      "http://news.donga.com/Politics\n",
      "error!!! http://news.donga.com/Politics/3/00030300000001/20181010/92325103/1\n",
      "http://news.donga.com/Economy\n",
      "error!!! http://news.donga.com/Economy/3/70010000000919/20181010/92325051/1\n",
      "http://news.donga.com/Sports\n",
      "error!!! http://news.donga.com/Top/3/all/20181010/92325081/1\n",
      "http://news.donga.com/Enter\n",
      "error!!! http://news.donga.com/Top/3/all/20181010/92325179/1\n",
      "http://news.donga.com/Society\n",
      "error!!! http://news.donga.com/Society/3/030125/20181004/92246465/1\n",
      "http://news.donga.com/Inter\n",
      "error!!! http://news.donga.com/Inter/3/021605/20181010/92332052/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_weight done!\n",
      "153.59699320793152\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    startTime = time.time()\n",
    "    tfidf_raw = main()\n",
    "    endTime = time.time()\n",
    "    print(endTime - startTime)\n",
    "    \n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight = cal_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
