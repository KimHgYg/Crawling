{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from datetime import datetime\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "import nltk,  operator, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Twitter, Kkma\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import *\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    _html = \"\"\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding='utf-8'\n",
    "    if resp.status_code == 200 :\n",
    "        _html = resp.text\n",
    "    return _html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect mongoDB\n",
    "\n",
    "def init_mongo():\n",
    "    global Conn\n",
    "    Conn = MongoClient('52.79.249.174',27017)\n",
    "    global db\n",
    "    db = Conn.crawling\n",
    "    global background\n",
    "    background = db.background\n",
    "    global foreground\n",
    "    foreground = db.foreground\n",
    "    global user_tfdif\n",
    "    user_tfdif = db.user_tfidf\n",
    "    #Conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정치 경제 스포츠 연예 사회 국제\n",
    "#politics economy sports enter national inter\n",
    "#   0         1     2     3      4       5\n",
    "#조선일보 경제는 biz.chosun.com...\n",
    "CHOSUN = [(\"http://news.chosun.com/politics\",'politics',0),(\"http://news.chosun.com/sports\",'sports',2), (\"http://news.chosun.com/ent\",'enter',3), (\"http://news.chosun.com/national\",'national',4),( \"http://news.chosun.com/international\",'inter',5)]\n",
    "\n",
    "#동아일보는 패턴 통일\n",
    "DONGA = [(\"http://news.donga.com/Politics\",'politics',0),(\"http://news.donga.com/Economy\",'economy',1),(\"http://news.donga.com/Sports\",'sports',2),(\"http://news.donga.com/Enter\",'enter',3),(\"http://news.donga.com/Society\",'national',4),(\"http://news.donga.com/Inter\",'inter',5)]\n",
    "\n",
    "#중앙일보 연예가 가요, 방송, 영화로 나눠져있음\n",
    "JOONGANG = [(\"http://news.joins.com/politics\",'politics',0),(\"http://news.joins.com/money\",'economy',1),(\"http://news.joins.com/sports\",'sports',2),(\"http://news.joins.com/society\",'national',4),(\"http://news.joins.com/world\",'inter',5)]\n",
    "JOONGANG_CUL = [(\"http://news.joins.com/culture/song/list\",'enter',3),(\"http://news.joins.com/culture/broadcast/list\",'enter',3),(\"http://news.joins.com/culture/movie/list\",'enter',3)]\n",
    "\n",
    "#기사 내용을 담을 변수\n",
    "try:\n",
    "    _id = foreground.find().sort([('_id',pymongo.DESCENDING)])[0]['_id']\n",
    "except:\n",
    "    _id = 0\n",
    "num_press = 3\n",
    "num_class = 6\n",
    "docs = [[] for i in range(num_class)]\n",
    "\n",
    "lock = asyncio.Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    중앙일보 날짜 가져오기 힘들어서 datetime 패키지를 이용하여 임의로 날짜 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chosun_crawling():\n",
    "    data_foreground = []\n",
    "    i = 0\n",
    "    global _id\n",
    "    for clas in CHOSUN:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        for lt in soup.find_all(\"dt\") :\n",
    "            try :\n",
    "                tmp = lt.find('a')\n",
    "                link = tmp.get('href')\n",
    "                day = link.split('/')\n",
    "                day = day[6]+day[7]+day[8]\n",
    "                text = tmp.get_text()\n",
    "                data_foreground.append({'_id': str(_id).zfill(10), 'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day, 'class' : clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class' : 'par'}).get_text(),link), _id)\n",
    "                _id+=1\n",
    "            except : \n",
    "                print('error!!!',link)\n",
    "        #docs[i].append(tmp_list)\n",
    "        i = i + 1\n",
    "        if i == 1 :\n",
    "            i = i + 1\n",
    "    foreground.insert_many(data_foreground)\n",
    "    print('{0} done!',format(current_process().name))\n",
    "    return\n",
    "\n",
    "def chosun_biz():\n",
    "    html = get_html('http://biz.chosun.com')\n",
    "    print('http://biz.chosun.com')\n",
    "    soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "    data_foreground = []\n",
    "    head = soup.find(\"div\", attrs={'class' : 'mt_art_tit'})\n",
    "    tmp = head.find('a')\n",
    "    link = tmp.get('href')\n",
    "    day = link.split('/')\n",
    "    day = day[6]+day[7]+day[8]\n",
    "    text = tmp.get_text()\n",
    "    global _id\n",
    "    j=0\n",
    "    data_foreground.append({'_id': str(_id).zfill(10), 'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day, 'class' : 'economy'})\n",
    "    \n",
    "    html = get_html(link)\n",
    "    soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "    docs[1].append((soup2.find('div',id = 'article_2011', attrs={'class':'article'}).get_text(),link),_id)\n",
    "    _id+=1\n",
    "    art = soup.find(\"div\", attrs={'class' : 'mc_art_lst'})\n",
    "    for lt in art.find_all(\"li\", limit = 30) :\n",
    "        try :\n",
    "            tmp = lt.find('a')\n",
    "            #print(tmp)\n",
    "            link = tmp.get('href')\n",
    "            day = link.split('/')\n",
    "            day = day[6]+day[7]+day[8]\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'_id':str(_id).zfill(10), 'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day,'class':'economy'})\n",
    "            \n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[1].append((soup2.find('div',id = 'article_2011', attrs={'class':'article'}).get_text(),link),_id)\n",
    "            _id+=1\n",
    "        except : \n",
    "            print('error!!!',link)\n",
    "    foreground.insert_many(data_foreground)\n",
    "    print('{0} done!',format(current_process().name))\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donga_crawling():\n",
    "    data_foreground = []\n",
    "    #docs = []\n",
    "    i = 0\n",
    "    global _id\n",
    "    for clas in donga:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        try:\n",
    "            tmp = soup.find('div',attrs={'class':'articleTop'})\n",
    "            head = tmp.find(\"div\", attrs={'class' : 'articleMain'})\n",
    "            #해드라인_main\n",
    "            head_main = head.find('a')\n",
    "            link = head_main.get('href')\n",
    "            day = link.split('/')[6]\n",
    "            text = head.find(attrs={'class' : 'title'}).get_text()\n",
    "            data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            \n",
    "            #print(data_foreground)\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link),_id)\n",
    "            _id+=1\n",
    "            for head_sub in tmp.find_all('li') :\n",
    "                art = head_sub.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                \n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link),_id)\n",
    "                _id+=1\n",
    "\n",
    "        except :\n",
    "            for head in tmp.find_all('div',attrs={'class':'artivleMain'}):\n",
    "                art = head.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                \n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link),_id)\n",
    "                _id+=1\n",
    "                \n",
    "            for head in tmp.find_all('div',attrs={'class':'artivleMain02'}):\n",
    "                art = head.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                \n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link),_id)\n",
    "                _id+=1\n",
    "\n",
    "        try:\n",
    "            #issue\n",
    "            content_issue = soup.find('div',attrs={'class':'issueList'})\n",
    "            issue = content_issue.find('a', attrs={'class' : 'tit'})\n",
    "            link = issue.get('href')\n",
    "            day = link.split('/')[6]\n",
    "            text = issue.get_text();\n",
    "            data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            \n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div',attrs={'class':'atrticle_txt'}).get_text(),link),_id)\n",
    "            _id+=1\n",
    "            for issue_sub in content_issue.find_all('li') :\n",
    "                art = issue_sub.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.get_text()\n",
    "                data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                \n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link),_id)\n",
    "                _id+=1\n",
    "        except :\n",
    "            print('error!!!',link)\n",
    "            \n",
    "        try:\n",
    "            #최신기사\n",
    "            contents = soup.find('div',attrs={'class':'articleList_con'})\n",
    "            for art in contents.find_all('div',attrs={'class':'rightList'}) :\n",
    "                tmp = art.find('a')\n",
    "                link = tmp.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = tmp.find(attrs={'class':'tit'}).get_text()\n",
    "                data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                \n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link),_id)\n",
    "                _id+=1\n",
    "        except :\n",
    "            print('error!!!',link)\n",
    "        i = i + 1\n",
    "\n",
    "    foreground.insert_many(data_foreground)\n",
    "    print('{0} done!',format(current_process().name))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joongang_crawling():\n",
    "    day = datetime.today().strftime(\"%Y%m%d\")\n",
    "    data_foreground = []\n",
    "    i = 0\n",
    "    global _id\n",
    "    for clas in JOONGANG:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        origin = soup.find('div',id='content')\n",
    "        #해드라인\n",
    "        try:\n",
    "            art = origin.find('dt')\n",
    "            tmp = art.find('a')\n",
    "            link = tmp.get('href')\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            \n",
    "            html2 = get_html(link)\n",
    "            soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link),_id)\n",
    "            _id+=1\n",
    "        except :\n",
    "            #sport 해드라인\n",
    "            try:\n",
    "                for art in origin.find_all('div',attrs={'class':'slide'}):\n",
    "                    tmp = art.find(attrs={'class':'headline mg'})\n",
    "                    tmp2 = tmp.find('a')\n",
    "                    link = tmp2.get('href')\n",
    "                    text = tmp2.get_text();\n",
    "                    data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                    \n",
    "                    html2 = get_html(link)\n",
    "                    soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                    docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link),_id)\n",
    "                    _id+=1\n",
    "            except:\n",
    "                print('error!!!',link)\n",
    "        \n",
    "        #실시간 주요뉴스\n",
    "        try:\n",
    "            art = origin.find('div',attrs={'class':'default_realtime'})\n",
    "            art = art.find('ul',id='ulItems')\n",
    "            #print(art)\n",
    "            for content in art.find_all('li'):\n",
    "                #print(content)\n",
    "                tmp = content.find(attrs={'class':'headline mg'})\n",
    "                tmp2 = tmp.find('a')\n",
    "                link = 'http://news.joins.com/' + tmp2.get('href')\n",
    "                text = tmp2.get_text()\n",
    "                data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                \n",
    "                html2 = get_html(link)\n",
    "                soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link),_id)\n",
    "                _id+=1\n",
    "        except:\n",
    "            try:\n",
    "                art = origin.find('div',attrs={'class':'combination_today'})\n",
    "                art = art.find('div',attrs={'class':'bd'})\n",
    "                for content in art.find_all('li'):\n",
    "                    tmp = content.find(attrs={'class':'headline mg'})\n",
    "                    tmp2 = tmp.find('a')\n",
    "                    link = 'http://news.joins.com/' + tmp2.get('href')\n",
    "                    text = tmp2.get_text()\n",
    "                    data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                    \n",
    "                    html2 = get_html(link)\n",
    "                    soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                    docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link),_id)\n",
    "                    _id+=1\n",
    "            except :\n",
    "                print('error!!!',link)\n",
    "        i = i + 1\n",
    "        if i == 3 :\n",
    "            i = i + 1\n",
    "    foreground.insert_many(data_foreground)\n",
    "    print('{0} done!',format(current_process().name))\n",
    "    return \n",
    "\n",
    "def joongang_cul():\n",
    "    day = datetime.today().strftime(\"%Y%m%d\")\n",
    "    dosc = []\n",
    "    global _id\n",
    "    #tmp_list = []\n",
    "    data_foreground = []\n",
    "    for clas in JOONGANG_CUL:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        \n",
    "        origin = soup.find('div',id='content')\n",
    "        for content in origin.find_all('li'):\n",
    "            art = content.find(attrs={'class':'headline mg'})\n",
    "            tmp = art.find('a')\n",
    "            link = 'http://news.joins.com' + tmp.get('href')\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'_id':str(_id).zfill(10),'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html2 = get_html(link)\n",
    "            soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "            docs[3].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link),_id)\n",
    "            _id+=1\n",
    "    foreground.insert_many(data_foreground)\n",
    "    print('{0} done!',format(current_process().name))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    scikit-Learn 패키지를 이용한 TF-IDF 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['.',',','\\n','\\xa0',re.compile('^A-Za-z*$')]\n",
    "\n",
    "\n",
    "def cal_weight():\n",
    "    print('calculating weight...')\n",
    "    tfidf_list = []\n",
    "    tfidf_list_raw = []\n",
    "    t = Twitter()\n",
    "    k = Kkma()\n",
    "    for i in range(num_class):\n",
    "    #for i in range(1):\n",
    "        nouns = []\n",
    "        for article in docs[i]:\n",
    "            if article[0] is not '':\n",
    "                nouns.append(' '.join([noun for noun in t.nouns(str(article[0]))]))        \n",
    "        vec = TfidfVectorizer(stop_words = stopwords)\n",
    "        fitted = vec.fit(nouns)\n",
    "        tfidf_res = fitted.transform(nouns)\n",
    "        vocab = fitted.get_feature_names()\n",
    "        j = 0\n",
    "        for article in tfidf_res.toarray():\n",
    "            idf = sorted(zip(vocab,article), key=lambda kv:kv[1])[-3:]\n",
    "            tmp = idf[0][0]+' '+idf[1][0] + ' ' + idf[2][0]\n",
    "            tfidf_list.append({'_id':docs[i][j][2],'tfidf':tmp,'link':docs[i][j][1]})\n",
    "            j = j + 1\n",
    "    background.insert_many(tfidf_list)\n",
    "    \n",
    "    print('cal_weight done!')\n",
    "    return tfidf_list_raw"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    DB에 있는 사용자 기록을 학습하는 학습 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    #load model\n",
    "    def loadModel():\n",
    "        a = False\n",
    "        date = input(\"input date you want to load model format : (yyyymmdd) : \")\n",
    "        self.model = load_model('relu_model'+date)\n",
    "        self.sig_model = load_model('sigmoid_model'+date)\n",
    "        self.model_bi = load_model('bi_relu_model'+date)\n",
    "        self.model_sig_bi = load_model('bi_sigmoid_model'+date)\n",
    "        if(self.model):\n",
    "            print('relu model loaded')\n",
    "            a = True\n",
    "        else:\n",
    "            print('No model relu')\n",
    "        if(self.sig_model):\n",
    "            print('sigmoid model loaded')\n",
    "            a = True\n",
    "        else:\n",
    "            print('No model sigmoid')\n",
    "        if(self.model_bi):\n",
    "            print('bi relu model loaded')\n",
    "            a = True\n",
    "        else:\n",
    "            print('No model bi relu')\n",
    "            \n",
    "        if(self.model_sig_bi):\n",
    "            print('bi sigmoid model loaded')\n",
    "            a = True\n",
    "        else:\n",
    "            print('No model bi sigmoid')\n",
    "            \n",
    "        return a\n",
    "    \n",
    "    # bidirectional 모델 만들기!\n",
    "    #activate = 'tanh'\n",
    "    #kernel_init = 'glorot_uniform'\n",
    "    #time_step,  output_shape, model_name_to_save\n",
    "    #number of feature = 8\n",
    "    split_ratio = 0.8\n",
    "    max_pad = 15\n",
    "    def build_model(self, max_pad, categori_shape):\n",
    "        activate = 'relu'\n",
    "        kernel_init = 'Orthogonal'\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(256, return_sequences=True, input_shape=(max_pad,8), activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(categori_shape, activation='sigmoid', kernel_initializer = 'Orthogonal'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def build_model_sigmoid(self, max_pad, categori_shape):\n",
    "        activate_1 = 'sigmoid'\n",
    "        kernel_init = 'glorot_uniform'\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(256, return_sequences=True, input_shape=(max_pad,8), activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(categori_shape, activation=activate_1, kernel_initializer = kernel_init))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def build_model_bi(self, max_pad, categori_shape):\n",
    "        activate = 'relu'\n",
    "        kernel_init = 'Orthogonal'\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init), input_shape=(max_pad,8)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(categori_shape, activation='sigmoid', kernel_initializer = 'Orthogonal'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def build_model_sigmoid_bi(self, max_pad, categori_shape):\n",
    "        activate_1 = 'sigmoid'\n",
    "        kernel_init = 'glorot_uniform'\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init), input_shape=(max_pad,8)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(categori_shape, activation=activate_1, kernel_initializer = kernel_init))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def learning(self, model, train_X, train_Y, test_X, test_Y, model_name):\n",
    "        history = model.fit(train_X,train_Y, epochs=150, batch_size=1, validation_split=0.1)\n",
    "        score = model.evaluate(test_X, test_Y)\n",
    "        model.save(datetime.today().strftime(\"%Y%m%d\")+model_name)\n",
    "        return (history, score)\n",
    "    \n",
    "    def plt_init(self):\n",
    "        plt.style.use('bmh')\n",
    "\n",
    "    #plot 그리기\n",
    "    def draw_plot(self, history, score, name):\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.title(name)\n",
    "        plt.plot(history.history['loss'], label = name)\n",
    "        plt.legend()\n",
    "        plt.plot(history.history['val_loss'], label = 'val_'+name)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(datetime.today().strftime(\"%Y%m%d\")+'_'+name, bbox_inches='tight')\n",
    "        print(\"loss over test data : %.2f\" % (score[0]))\n",
    "        print(\"accuracy over test data : %.2f\" % (score[1]))\n",
    "\n",
    "    def data_manipulate(self):\n",
    "        print('Creating training set...')\n",
    "\n",
    "        \n",
    "        #db에서 데이터 불러오기\n",
    "        user_tfidf_info = user_tfidf.find()\n",
    "        user_pri_info = user_pri.find()\n",
    "\n",
    "        #tokenizer \n",
    "        token = Tokenizer()\n",
    "\n",
    "        #db에서 불러온 데이터 dataframe형태로 저장\n",
    "        user_tfidf = pd.DataFrame(user_tfidf_info,columns = ['ID','tfidf'])\n",
    "        user_pri = pd.DataFrame(user_pri_info,columns = ['ID','age','gender','inter1','inter2','inter3'])\n",
    "\n",
    "        #tfidf 분할을 위해 따로 drop\n",
    "        user_tfidf_value = user_tfidf.drop(columns = ['ID']).values\n",
    "        user_tfidf = user_tfidf.drop(['tfidf'],axis=1)\n",
    "\n",
    "        i = 0\n",
    "        tmp = []\n",
    "        #tfidf 분할\n",
    "        for arg in user_tfidf_value:\n",
    "            tmp.append(' '.join(arg).split(' '))\n",
    "\n",
    "        #tokenizer 학습\n",
    "        token.fit_on_texts(tmp)\n",
    "\n",
    "        tmp = token.texts_to_sequences(tmp)\n",
    "        \n",
    "        self.idx_word = {}\n",
    "        \n",
    "        for w in token.word_index : \n",
    "            self.idx_word[token.word_index[w]] = w\n",
    "\n",
    "        #분할 한 tfidf dataframe으로 만든 후 원본과 합친다\n",
    "        tmp = pd.DataFrame(tmp,columns = ['tfidf','tfidf2','tfidf3'])\n",
    "\n",
    "        user_tfidf = pd.concat([user_tfidf,tmp],axis=1)\n",
    "\n",
    "        #tfidf와 사용자 정보를 merge\n",
    "        merged = pd.merge(user_tfidf,user_pri)\n",
    "\n",
    "        #ID별로 Y 구하기 위해 추출\n",
    "        ID = user_tfidf.ID.unique()\n",
    "\n",
    "        train_X = []\n",
    "        train_Y = []\n",
    "        \n",
    "        #train_X max_pad 만큼 ID별로 데이터 추가하거나 삭제\n",
    "        for id in ID:\n",
    "            tmp = merged.loc[merged['ID'] == id,'tfidf':].values.T\n",
    "            tmp = pad_sequences(tmp,maxlen = self.max_pad).T\n",
    "            train_X.extend(tmp)\n",
    "\n",
    "        train_X = np.array(train_X)[:,np.newaxis].reshape(-1,self.max_pad,8)\n",
    "\n",
    "        #ID별로 train_Y\n",
    "\n",
    "        for id in ID:\n",
    "            user_private = merged.loc[merged['ID'] == id, 'tfidf' : 'tfidf3' ].values\n",
    "            user_private = np.vstack((user_private[1:],user_private[0])).T\n",
    "            user_private = pad_sequences(user_private, maxlen = self.max_pad).T\n",
    "            train_Y.extend(user_private)\n",
    "\n",
    "        tmp = []\n",
    "        for target in to_categorical(np.array(train_Y).reshape(-1,3)):\n",
    "            tmp.append(sum(target))\n",
    "\n",
    "        train_Y = np.array(tmp)[:,np.newaxis].reshape(-1,self.max_pad,len(tmp[0]))\n",
    "        #print(train_Y.shape)\n",
    "\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        print('training set created, ready to build model')\n",
    "        return (train_X, train_Y)\n",
    "    \n",
    "    def build_learn_all(self):\n",
    "        #모델 구축\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model = self.build_model(self.max_pad, self.train_Y.shape[2])\n",
    "        self.model_sig = self.build_model_sigmoid(self.max_pad, self.train_Y.shape[2])\n",
    "        self.model_bi = self.build_model_bi(self.max_pad, self.train_Y.shape[2])\n",
    "        self.model_sig_bi = self.build_model_sigmoid_bi(self.max_pad, self.train_Y.shape[2])\n",
    "        \n",
    "        #모델 run\n",
    "        (history, score) = self.learning(self.model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        (history2, score2) = self.learning(self.model_sig, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_sigmoid')\n",
    "        (history3, score3) = self.learning(self.model_bi, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu_bi')\n",
    "        (history4, score4) = self.learning(self.model_sig_bi, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_sigmoid_bi')\n",
    "        #plot iniit\n",
    "        plt = self.plt_init()\n",
    "\n",
    "        #plot 그리기\n",
    "        self.draw_plot(history, score, '_relu')\n",
    "        self.draw_plot(history2, score2,'_sigmoid')\n",
    "        self.draw_plot(history3, score3,'_relu_bi')\n",
    "        self.draw_plot(history4, score4,'_sigmoid_bi')\n",
    "\n",
    "        return (self.model, self.model_sig, self.model_bi, self.model_sig_bi);\n",
    "    \n",
    "    def build_learn_model(self):\n",
    "        print('building relu model...')\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model = self.build_model(self.max_pad, self.train_Y.shape[2])\n",
    "        (history, score) = self.learning(model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        plt = self.plt_init()\n",
    "        self.draw_plot(history, score, '_relu')\n",
    "        self.model.save('relu_model'+datetime.today().strftime(\"%Y%m%d\"))\n",
    "        print('relu model done')\n",
    "        return self.model\n",
    "    \n",
    "    def build_learn_model_sig(self):\n",
    "        print('building sigmoid model')\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model_sig = self.build_model_sigmoid(self.max_pad, self.train_Y.shape[2])\n",
    "        (history, score) = self.learning(model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        plt = self.plt_init()\n",
    "        self.draw_plot(history, score, '_sigmoid')\n",
    "        self.model.save('sigmoid_model'+datetime.today().strftime(\"%Y%m%d\"))\n",
    "        print('sigmoid model done')\n",
    "        return self.model_sig\n",
    "    \n",
    "    def build_learn_model_bi(self):\n",
    "        print('building bidirectional relu model')\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model_bi = self.build_model_bi(self.max_pad, self.train_Y.shape[2])\n",
    "        (history, score) = self.learning(model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        plt = self.plt_init()\n",
    "        self.draw_plot(history, score, '_relu_bi')\n",
    "        self.model.save('bi_relu_model')\n",
    "        print('bidirectional relu model done'+datetime.today().strftime(\"%Y%m%d\"))\n",
    "        return self.model_bi\n",
    "    \n",
    "    def build_learn_model_sig_bi(self):\n",
    "        print('building bidirectional sigmoid model')\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model_sig_bi = self.build_model_sig_bi(self.max_pad, self.train_Y.shape[2])\n",
    "        (history, score) = self.learning(model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        plt = self.plt_init()\n",
    "        self.draw_plot(history, score, '_sigmoid_bi')\n",
    "        self.model.save('bi_sigmoid_model'+datetime.today().strftime(\"%Y%m%d\"))\n",
    "        print('sigmoid model done')\n",
    "        return self.model_sig_bi\n",
    "    \n",
    "    def predict_all(self):\n",
    "        test = []\n",
    "        ret = []\n",
    "        pred = self.model.predict()\n",
    "        ret.append(np.argmax(pred, axis=0))\n",
    "        pred = self.model_sig.predict()\n",
    "        ret.append(np.argmax(pred, axis=0))\n",
    "        pred = self.model_bi.predict()\n",
    "        ret.append(np.argmax(pred, axis=0))\n",
    "        pred = self.model_sig_bi.predict()\n",
    "        ret.append(np.argmax(pred, axis=0))\n",
    "        return ret\n",
    "    \n",
    "    def precit_model(self):\n",
    "        pred = self.model.predict()\n",
    "        print(pred.shape)\n",
    "        return np.argmax(pred, axis=0)\n",
    "    \n",
    "    def precit_model_sig(self):\n",
    "        pred = self.model_sig.predict()\n",
    "        print(pred.shape)\n",
    "        return np.argmax(pred, axis=0)\n",
    "    \n",
    "    def precit_model_bi(self):\n",
    "        pred = self.model_bi.predict()\n",
    "        print(pred.shape)\n",
    "        return np.argmax(pred, axis=0)\n",
    "    \n",
    "    def precit_model_sig_bi(self):\n",
    "        pred = self.model_sig_bi.predict()\n",
    "        print(pred.shape)\n",
    "        return np.argmax(pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLP\\n    loadModel(self)\\n\\tdata_manipulate(self)\\n\\tbuild_model_all(self)\\n\\tbuild_learn_model(self)\\n\\tbuild_learn_model_sig(self)\\n\\tbuild_learn_model_bi(self)\\n\\tbuild_learn_model_sig_bi(self)\\n\\tpredict_all(self)\\n\\tpredict_model(self)\\n\\tpredict_model_sig(self)\\n\\tpredict_model_bi(self)\\n\\tpredict_model_sig_bi(self)\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"MLP\n",
    "    loadModel(self)\n",
    "\tdata_manipulate(self)\n",
    "\tbuild_model_all(self)\n",
    "\tbuild_learn_model(self)\n",
    "\tbuild_learn_model_sig(self)\n",
    "\tbuild_learn_model_bi(self)\n",
    "\tbuild_learn_model_sig_bi(self)\n",
    "\tpredict_all(self)\n",
    "\tpredict_model(self)\n",
    "\tpredict_model_sig(self)\n",
    "\tpredict_model_bi(self)\n",
    "\tpredict_model_sig_bi(self)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤링 각각 process 생성해서 돌린다\n",
    "\n",
    "def crawling(obj, logger):\n",
    "    print('Crawing starts...')\n",
    "    ps = []\n",
    "    crawling_list = [\n",
    "        [joongang_crawling, 'joongang_crawling'],\n",
    "        [joongang_cul,'joongang_cul'],\n",
    "        [chosun_crawling, 'chosun_crawling'],\n",
    "        [chosun_biz,'chosun_biz'],\n",
    "        [donga_crawling,'donga_crawglin']\n",
    "    ]\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        for func in crawling_list:\n",
    "            p = Process(target = func[0])#, name=func[1])\n",
    "            ps.append(p)\n",
    "            p.start()\n",
    "\n",
    "        for p in ps:\n",
    "            p.join()\n",
    "\n",
    "    cal_weight()\n",
    "    obj.data_manipulate()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_program():\n",
    "    multiprocessing.log_to_stderr()\n",
    "    logger = multiprocessing.get_logger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    obj = MLP()\n",
    "    init_mongo()\n",
    "    return (obj, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base(obj, logger):\n",
    "    #init_mysql()\n",
    "    multiprocessing.log_to_stderr()\n",
    "    logger = multiprocessing.get_logger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    build_ready = False\n",
    "    recommand_ready = False\n",
    "    p = None\n",
    "    #  Menu\n",
    "    while(1):\n",
    "        menu = int(input(\"1 : crawling \" +\n",
    "                         \"2 : load model\" +\n",
    "                         \"3 : build model \"+\n",
    "                         \"4 : start recommand program\"))\n",
    "        #  start crawling -- blocked\n",
    "        if(menu == 1):\n",
    "            train_ready = crawling(obj, logger)\n",
    "            \n",
    "        #    load model --blocked\n",
    "        elif(menu == 2):\n",
    "            if(obj.loadModel()==False):\n",
    "                print(\"There's no saved model, please build model first\")\n",
    "                \n",
    "        # build model -- blocked\n",
    "        elif(menu == 3):\n",
    "            if(train_ready == False):\n",
    "                print(\"Training set not exist\")\n",
    "                continue\n",
    "                \n",
    "            p = int(input(\"1: build relu model\\n\" + \n",
    "                     \"2 : build sigmoid model\\n\" +\n",
    "                     \"3 : build bidirectional relu model\\n\" +\n",
    "                     \"4 : build bidirectional sigmoid model\\n\" +\n",
    "                     \"5 : build all model\\n\"))\n",
    "            if(p == 1):\n",
    "                p = Process(target=obj.build_learn_model)\n",
    "                p.start()\n",
    "            elif(p == 2):\n",
    "                p = Process(target=obj.build_learn_model_sig)\n",
    "                p.start()\n",
    "            elif(p == 3):\n",
    "                p = Process(target=obj.build_learn_model_bi)\n",
    "                p.start()\n",
    "            elif(p == 4):\n",
    "                p = Process(target=obj.build_learn_model_sig_bi)\n",
    "                p.start()\n",
    "            elif(p == 5):\n",
    "                p = Process(target=obj.build_model_all)\n",
    "                p.start()\n",
    "        #  start recommand program\n",
    "        elif(menu == 4):\n",
    "            if(p is None):\n",
    "                print('There is no model, please build model fisrt')\n",
    "                continue\n",
    "            subprocess.run([\"python\",\"./Recommand_v1.py\"], stdout=subprocess.PIPE\n",
    "                             , shell=True)\n",
    "            print('recommanding process start')\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : crawling 2 : load model3 : build model 4 : start recommand program1\n",
      "Crawing starts...\n",
      "calculating weight...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning:\n",
      "\n",
      "\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-804f245a5638>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_program\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mbase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-108-2909e243f542>\u001b[0m in \u001b[0;36mbase\u001b[1;34m(obj, logger)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#  start crawling -- blocked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmenu\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtrain_ready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#    load model --blocked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-8ec5ba8a0fa5>\u001b[0m in \u001b[0;36mcrawling\u001b[1;34m(obj, logger)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mcal_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_manipulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-4c6d41141ef6>\u001b[0m in \u001b[0;36mcal_weight\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mnouns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnoun\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnoun\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mfitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mtfidf_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m-> 1361\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    812\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    (obj, logger) = init_program()\n",
    "    base(obj, logger)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
