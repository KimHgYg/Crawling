{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import mysql.connector\n",
    "import sshtunnel\n",
    "from mysql.connector import errorcode\n",
    "from time import sleep\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk,  operator, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Twitter, Kkma\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import *\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mysql():\n",
    "    global cnx\n",
    "    cnx = mysql.connector.connect(user='user1', password='tkfkdgody1!', host='52.79.185.101', database='Crawling')\n",
    "    global cursor\n",
    "    cursor = cnx.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "def conn_nodejs():\n",
    "    with socket.socket() as s:\n",
    "        s.connect('13.124.207.176',3002)\n",
    "        while(1):\n",
    "            print\n",
    "            \n",
    "conn_nodejs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_foreground = (\"INSERT IGNORE INTO foreground (title, press, link, day, class) \"\n",
    "                  \"VALUES (%(title)s, %(press)s, %(link)s, %(day)s, %(class)s)\")\n",
    "add_background = (\"INSERT IGNORE INTO background (tfidf,link) \"\n",
    "                 \"VALUES (%(tfidf)s, %(link)s)\")\n",
    "clear_db = (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    _html = \"\"\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding='utf-8'\n",
    "    if resp.status_code == 200 :\n",
    "        _html = resp.text\n",
    "    return _html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정치 경제 스포츠 연예 사회 국제\n",
    "#politics economy sports enter national inter\n",
    "#조선일보 경제는 biz.chosun.com...\n",
    "CHOSUN = [(\"http://news.chosun.com/politics\",'politics'),(\"http://news.chosun.com/sports\",'sports'), (\"http://news.chosun.com/ent\",'enter'), (\"http://news.chosun.com/national\",'national'),( \"http://news.chosun.com/international\",'inter')]\n",
    "\n",
    "#동아일보는 패턴 통일\n",
    "DONGA = [(\"http://news.donga.com/Politics\",'politics'),(\"http://news.donga.com/Economy\",'economy'),(\"http://news.donga.com/Sports\",'sports'),(\"http://news.donga.com/Enter\",'enter'),(\"http://news.donga.com/Society\",'national'),(\"http://news.donga.com/Inter\",'inter')]\n",
    "\n",
    "#중앙일보 연예가 가요, 방송, 영화로 나눠져있음\n",
    "JOONGANG = [(\"http://news.joins.com/politics\",'politics'),(\"http://news.joins.com/money\",'economy'),(\"http://news.joins.com/sports\",'sports'),(\"http://news.joins.com/society\",'national'),(\"http://news.joins.com/world\",'inter')]\n",
    "JOONGANG_CUL = [(\"http://news.joins.com/culture/song/list\",'enter'),(\"http://news.joins.com/culture/broadcast/list\",'enter'),(\"http://news.joins.com/culture/movie/list\",'enter')]\n",
    "\n",
    "#기사 내용을 담을 변수\n",
    "\n",
    "num_press = 3\n",
    "num_class = 6\n",
    "docs = [[] for i in range(num_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    중앙일보 날짜 가져오기 힘들어서 datetime 패키지를 이용하여 임의로 날짜 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chosun_crawling():\n",
    "    data_foreground = []\n",
    "    i = 0\n",
    "    for clas in CHOSUN:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        for lt in soup.find_all(\"dt\") :\n",
    "            try :\n",
    "                tmp = lt.find('a')\n",
    "                link = tmp.get('href')\n",
    "                day = link.split('/')\n",
    "                day = day[6]+day[7]+day[8]\n",
    "                text = tmp.get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day, 'class' : clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class' : 'par'}).get_text(),link))\n",
    "            except : \n",
    "                print('error!!!',link)\n",
    "        #docs[i].append(tmp_list)\n",
    "        i = i + 1\n",
    "        if i == 1 :\n",
    "            i = i + 1\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return\n",
    "\n",
    "def chosun_biz():\n",
    "    html = get_html('http://biz.chosun.com')\n",
    "    print('http://biz.chosun.com')\n",
    "    soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "    data_foreground = []\n",
    "    head = soup.find(\"div\", attrs={'class' : 'mt_art_tit'})\n",
    "    tmp = head.find('a')\n",
    "    link = tmp.get('href')\n",
    "    day = link.split('/')\n",
    "    day = day[6]+day[7]+day[8]\n",
    "    text = tmp.get_text()\n",
    "    data_foreground.append({'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day, 'class' : 'economy'})\n",
    "    \n",
    "    html = get_html(link)\n",
    "    soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "    docs[1].append((soup2.find('div',id = 'article_2011', attrs={'class':'article'}).get_text(),link))\n",
    "    \n",
    "    art = soup.find(\"div\", attrs={'class' : 'mc_art_lst'})\n",
    "    for lt in art.find_all(\"li\", limit = 30) :\n",
    "        try :\n",
    "            tmp = lt.find('a')\n",
    "            #print(tmp)\n",
    "            link = tmp.get('href')\n",
    "            day = link.split('/')\n",
    "            day = day[6]+day[7]+day[8]\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day,'class':'economy'})\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[1].append((soup2.find('div',id = 'article_2011', attrs={'class':'article'}).get_text(),link))\n",
    "        except : \n",
    "            print('error!!!',link)\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donga_crawling():\n",
    "    data_foreground = []\n",
    "    #docs = []\n",
    "    i = 0\n",
    "    for clas in DONGA:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        try:\n",
    "            tmp = soup.find('div',attrs={'class':'articleTop'})\n",
    "            head = tmp.find(\"div\", attrs={'class' : 'articleMain'})\n",
    "            #해드라인_main\n",
    "            head_main = head.find('a')\n",
    "            link = head_main.get('href')\n",
    "            day = link.split('/')[6]\n",
    "            text = head.find(attrs={'class' : 'title'}).get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            #print(data_foreground)\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "            for head_sub in tmp.find_all('li') :\n",
    "                art = head_sub.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "\n",
    "        except :\n",
    "            for head in tmp.find_all('div',attrs={'class':'artivleMain'}):\n",
    "                art = head.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "\n",
    "                \n",
    "            for head in tmp.find_all('div',attrs={'class':'artivleMain02'}):\n",
    "                art = head.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "\n",
    "        try:\n",
    "            #issue\n",
    "            content_issue = soup.find('div',attrs={'class':'issueList'})\n",
    "            issue = content_issue.find('a', attrs={'class' : 'tit'})\n",
    "            link = issue.get('href')\n",
    "            day = link.split('/')[6]\n",
    "            text = issue.get_text();\n",
    "            data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div',attrs={'class':'atrticle_txt'}).get_text(),link))\n",
    "            for issue_sub in content_issue.find_all('li') :\n",
    "                art = issue_sub.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "        except :\n",
    "            print('error!!!',link)\n",
    "            \n",
    "        try:\n",
    "            #최신기사\n",
    "            contents = soup.find('div',attrs={'class':'articleList_con'})\n",
    "            for art in contents.find_all('div',attrs={'class':'rightList'}) :\n",
    "                tmp = art.find('a')\n",
    "                link = tmp.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = tmp.find(attrs={'class':'tit'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "        except :\n",
    "            print('error!!!',link)\n",
    "        i = i + 1\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joongang_crawling():\n",
    "    day = datetime.today().strftime(\"%Y%m%d\")\n",
    "    data_foreground = []\n",
    "    i = 0\n",
    "    for clas in JOONGANG:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        origin = soup.find('div',id='content')\n",
    "        #해드라인\n",
    "        try:\n",
    "            art = origin.find('dt')\n",
    "            tmp = art.find('a')\n",
    "            link = tmp.get('href')\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html2 = get_html(link)\n",
    "            soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "        except :\n",
    "            #sport 해드라인\n",
    "            try:\n",
    "                for art in origin.find_all('div',attrs={'class':'slide'}):\n",
    "                    tmp = art.find(attrs={'class':'headline mg'})\n",
    "                    tmp2 = tmp.find('a')\n",
    "                    link = tmp2.get('href')\n",
    "                    text = tmp2.get_text();\n",
    "                    data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                    html2 = get_html(link)\n",
    "                    soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                    docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "            except:\n",
    "                print('error!!!',link)\n",
    "        \n",
    "        #실시간 주요뉴스\n",
    "        try:\n",
    "            art = origin.find('div',attrs={'class':'default_realtime'})\n",
    "            art = art.find('ul',id='ulItems')\n",
    "            #print(art)\n",
    "            for content in art.find_all('li'):\n",
    "                #print(content)\n",
    "                tmp = content.find(attrs={'class':'headline mg'})\n",
    "                tmp2 = tmp.find('a')\n",
    "                link = 'http://news.joins.com/' + tmp2.get('href')\n",
    "                text = tmp2.get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html2 = get_html(link)\n",
    "                soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "        except:\n",
    "            try:\n",
    "                art = origin.find('div',attrs={'class':'combination_today'})\n",
    "                art = art.find('div',attrs={'class':'bd'})\n",
    "                for content in art.find_all('li'):\n",
    "                    tmp = content.find(attrs={'class':'headline mg'})\n",
    "                    tmp2 = tmp.find('a')\n",
    "                    link = 'http://news.joins.com/' + tmp2.get('href')\n",
    "                    text = tmp2.get_text()\n",
    "                    data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                    html2 = get_html(link)\n",
    "                    soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                    docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "            except :\n",
    "                print('error!!!',link)\n",
    "        i = i + 1\n",
    "        if i == 3 :\n",
    "            i = i + 1\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return \n",
    "\n",
    "def joongang_cul():\n",
    "    day = datetime.today().strftime(\"%Y%m%d\")\n",
    "    dosc = []\n",
    "    #tmp_list = []\n",
    "    data_foreground = []\n",
    "    for clas in JOONGANG_CUL:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        \n",
    "        origin = soup.find('div',id='content')\n",
    "        for content in origin.find_all('li'):\n",
    "            art = content.find(attrs={'class':'headline mg'})\n",
    "            tmp = art.find('a')\n",
    "            link = 'http://news.joins.com' + tmp.get('href')\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html2 = get_html(link)\n",
    "            soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "            docs[3].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    scikit-Learn 패키지를 이용한 TF-IDF 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['.',',','\\n','\\xa0',re.compile('^A-Za-z*$')]\n",
    "\n",
    "\n",
    "def cal_weight():\n",
    "    tfidf_list = []\n",
    "    tfidf_list_raw = []\n",
    "    t = Twitter()\n",
    "    k = Kkma()\n",
    "    for i in range(num_class):\n",
    "    #for i in range(1):\n",
    "        nouns = []\n",
    "        for article in docs[i]:\n",
    "            if article[0] is not '':\n",
    "                nouns.append(' '.join([noun for noun in t.nouns(str(article[0]))]))        \n",
    "        vec = TfidfVectorizer(stop_words = stopwords)\n",
    "        fitted = vec.fit(nouns)\n",
    "        tfidf_res = fitted.transform(nouns)\n",
    "        vocab = fitted.get_feature_names()\n",
    "        j = 0\n",
    "        for article in tfidf_res.toarray():\n",
    "            idf = sorted(zip(vocab,article), key=lambda kv:kv[1])[-3:]\n",
    "            tmp = idf[0][0]+' '+idf[1][0] + ' ' + idf[2][0]\n",
    "            tfidf_list.append({'tfidf':tmp,'link':docs[i][j][1]})\n",
    "            j = j + 1\n",
    "    cursor.executemany(add_background, tfidf_list)\n",
    "    cnx.commit()\n",
    "    cursor.execute('delete from foreground where link not in (select link from background);')\n",
    "    cnx.commit()\n",
    "    print('cal_weight done!')\n",
    "    return tfidf_list_raw"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    DB에 있는 사용자 기록을 학습하는 학습 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mysql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # bidirectional 모델 만들기!\n",
    "    #activate = 'tanh'\n",
    "    #kernel_init = 'glorot_uniform'\n",
    "    #time_step,  output_shape, model_name_to_save\n",
    "    \n",
    "    split_ratio = 0.8\n",
    "    max_pad = 15\n",
    "    def build_model(self, max_pad, categori_shape):\n",
    "        activate = 'relu'\n",
    "        kernel_init = 'Orthogonal'\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(256, return_sequences=True, input_shape=(max_pad,8), activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(categori_shape, activation='sigmoid', kernel_initializer = 'Orthogonal'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    def build_model_sigmoid(self, max_pad, categori_shape):\n",
    "        activate_1 = 'sigmoid'\n",
    "        kernel_init = 'glorot_uniform'\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(256, return_sequences=True, input_shape=(max_pad,8), activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(categori_shape, activation=activate_1, kernel_initializer = kernel_init))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def build_model_bi(self, max_pad, categori_shape):\n",
    "        activate = 'relu'\n",
    "        kernel_init = 'Orthogonal'\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init), input_shape=(max_pad,8)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(categori_shape, activation='sigmoid', kernel_initializer = 'Orthogonal'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def build_model_sigmoid_bi(self, max_pad, categori_shape):\n",
    "        activate_1 = 'sigmoid'\n",
    "        kernel_init = 'glorot_uniform'\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init), input_shape=(max_pad,8)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(256, return_sequences=True, activation=activate_1, kernel_initializer = kernel_init)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(categori_shape, activation=activate_1, kernel_initializer = kernel_init))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def learning(self, model, train_X, train_Y, test_X, test_Y, model_name):\n",
    "        history = model.fit(train_X,train_Y, epochs=150, batch_size=1, validation_split=0.1)\n",
    "        score = model.evaluate(test_X, test_Y)\n",
    "        model.save(datetime.today().strftime(\"%Y%m%d\")+model_name)\n",
    "        return (history, score)\n",
    "    \n",
    "    def plt_init(self):\n",
    "        plt.style.use('bmh')\n",
    "\n",
    "    #plot 그리기\n",
    "    def draw_plot(self, history, score, name):\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.title(name)\n",
    "        plt.plot(history.history['loss'], label = name)\n",
    "        plt.legend()\n",
    "        plt.plot(history.history['val_loss'], label = 'val_'+name)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(datetime.today().strftime(\"%Y%m%d\")+'_'+name, bbox_inches='tight')\n",
    "        print(\"loss over test data : %.2f\" % (score[0]))\n",
    "        print(\"accuracy over test data : %.2f\" % (score[1]))\n",
    "\n",
    "    def data_manipulate(self):\n",
    "        global cursor\n",
    "\n",
    "        #db에서 데이터 불러오기\n",
    "        cursor.execute('select * from user_tfidf')\n",
    "        user_tfidf_info = cursor.fetchall()\n",
    "        cursor.execute('select * from user_pri')\n",
    "        user_pri_info = cursor.fetchall()\n",
    "\n",
    "        #tokenizer \n",
    "        token = Tokenizer()\n",
    "\n",
    "        #db에서 불러온 데이터 dataframe형태로 저장\n",
    "        user_tfidf = pd.DataFrame(user_tfidf_info,columns = ['ID','tfidf'])\n",
    "        user_pri = pd.DataFrame(user_pri_info,columns = ['ID','age','gender','inter1','inter2','inter3'])\n",
    "\n",
    "        #tfidf 분할을 위해 따로 drop\n",
    "        user_tfidf_value = user_tfidf.drop(columns = ['ID']).values\n",
    "        user_tfidf = user_tfidf.drop(['tfidf'],axis=1)\n",
    "\n",
    "        i = 0\n",
    "        tmp = []\n",
    "        #tfidf 분할\n",
    "        for arg in user_tfidf_value:\n",
    "            tmp.append(' '.join(arg).split(' '))\n",
    "\n",
    "        #tokenizer 학습\n",
    "        token.fit_on_texts(tmp)\n",
    "\n",
    "        tmp = token.texts_to_sequences(tmp)\n",
    "        \n",
    "        self.idx_word = {}\n",
    "        \n",
    "        for w in token.word_index : \n",
    "            self.idx_word[token.word_index[w]] = w\n",
    "\n",
    "        #분할 한 tfidf dataframe으로 만든 후 원본과 합친다\n",
    "        tmp = pd.DataFrame(tmp,columns = ['tfidf','tfidf2','tfidf3'])\n",
    "\n",
    "        user_tfidf = pd.concat([user_tfidf,tmp],axis=1)\n",
    "\n",
    "        #tfidf와 사용자 정보를 merge\n",
    "        merged = pd.merge(user_tfidf,user_pri)\n",
    "\n",
    "        #ID별로 Y 구하기 위해 추출\n",
    "        ID = user_tfidf.ID.unique()\n",
    "\n",
    "        train_X = []\n",
    "        train_Y = []\n",
    "        \n",
    "        #train_X max_pad 만큼 ID별로 데이터 추가하거나 삭제\n",
    "        for id in ID:\n",
    "            tmp = merged.loc[merged['ID'] == id,'tfidf':].values.T\n",
    "            tmp = pad_sequences(tmp,maxlen = self.max_pad).T\n",
    "            train_X.extend(tmp)\n",
    "\n",
    "        train_X = np.array(train_X)[:,np.newaxis].reshape(-1,self.max_pad,8)\n",
    "\n",
    "        #train_X.reshape(-1,8,1)\n",
    "        #print(train_X.shape)\n",
    "\n",
    "        #ID별로 train_Y\n",
    "\n",
    "        for id in ID:\n",
    "            user_private = merged.loc[merged['ID'] == id, 'tfidf' : 'tfidf3' ].values\n",
    "            user_private = np.vstack((user_private[1:],user_private[0])).T\n",
    "            user_private = pad_sequences(user_private, maxlen = self.max_pad).T\n",
    "            train_Y.extend(user_private)\n",
    "\n",
    "        tmp = []\n",
    "        for target in to_categorical(np.array(train_Y).reshape(-1,3)):\n",
    "            tmp.append(sum(target))\n",
    "\n",
    "        train_Y = np.array(tmp)[:,np.newaxis].reshape(-1,self.max_pad,len(tmp[0]))\n",
    "        #print(train_Y.shape)\n",
    "\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        return (train_X, train_Y)\n",
    "    \n",
    "    def build_learn_all(self):\n",
    "        #모델 구축\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model = self.build_model(self.max_pad, self.train_Y.shape[2])\n",
    "        self.model_sig = self.build_model_sigmoid(self.max_pad, self.train_Y.shape[2])\n",
    "        self.model_bi = self.build_model_bi(self.max_pad, self.train_Y.shape[2])\n",
    "        self.model_sig_bi = self.build_model_sigmoid_bi(self.max_pad, self.train_Y.shape[2])\n",
    "\n",
    "        #모델 run\n",
    "        (history, score) = self.learning(self.model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        (history2, score2) = self.learning(self.model_sig, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_sigmoid')\n",
    "        (history3, score3) = self.learning(self.model_bi, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu_bi')\n",
    "        (history4, score4) = self.learning(self.model_sig_bi, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_sigmoid_bi')\n",
    "        #plot iniit\n",
    "        plt = self.plt_init()\n",
    "\n",
    "        #plot 그리기\n",
    "        self.draw_plot(history, score, '_relu')\n",
    "        self.draw_plot(history2, score2,'_sigmoid')\n",
    "        self.draw_plot(history3, score3,'_relu_bi')\n",
    "        self.draw_plot(history4, score4,'_sigmoid_bi')\n",
    "\n",
    "        return (self.model, self.model_sig, self.model_bi, self.model_sig_bi);\n",
    "    \n",
    "    def build_learn_model(self):\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model = self.build_model(self.max_pad, self.train_Y.shape[2])\n",
    "        (history, score) = self.learning(model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        plt = self.plt_init()\n",
    "        self.draw_plot(history, score, '_relu')\n",
    "        return self.model\n",
    "    \n",
    "    def build_learn_model_sig(self):\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model_sig = self.build_model_sigmoid(self.max_pad, self.train_Y.shape[2])\n",
    "        (history, score) = self.learning(model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        plt = self.plt_init()\n",
    "        self.draw_plot(history, score, '_sigmoid')\n",
    "        return self.model\n",
    "    \n",
    "    def build_learn_model_bi(self):\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model_bi = self.build_model_bi(self.max_pad, self.train_Y.shape[2])\n",
    "        (history, score) = self.learning(model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        plt = self.plt_init()\n",
    "        self.draw_plot(history, score, '_relu_bi')\n",
    "        return self.model\n",
    "    \n",
    "    def build_learn_model_sig_bi(self):\n",
    "        split = int(len(self.train_X)*self.split_ratio)\n",
    "        self.model_sig_bi = self.build_model_sig_bi(self.max_pad, self.train_Y.shape[2])\n",
    "        (history, score) = self.learning(model, self.train_X[:split],self.train_Y[:split],self.train_X[split:],self.train_Y[split:],'_relu')\n",
    "        plt = self.plt_init()\n",
    "        self.draw_plot(history, score, '_sigmoid_bi')\n",
    "        return self.model\n",
    "    \n",
    "    def predict_all(self):\n",
    "        test = []\n",
    "        ret = []\n",
    "        pred = self.model.predict()\n",
    "        ret.append(np.argmax(pred, axis=0))\n",
    "        pred = self.model_sig.predict()\n",
    "        ret.append(np.argmax(pred, axis=0))\n",
    "        pred = self.model_bi.predict()\n",
    "        ret.append(np.argmax(pred, axis=0))\n",
    "        pred = self.model_sig_bi.predict()\n",
    "        ret.append(np.argmax(pred, axis=0))\n",
    "        return ret\n",
    "    \n",
    "    def precit_model(self):\n",
    "        pred = self.model.predict()\n",
    "        return np.argmax(pred, axis=0)\n",
    "    \n",
    "    def precit_model_sig(self):\n",
    "        pred = self.model_sig.predict()\n",
    "        return np.argmax(pred, axis=0)\n",
    "    \n",
    "    def precit_model_bi(self):\n",
    "        pred = self.model_bi.predict()\n",
    "        return np.argmax(pred, axis=0)\n",
    "    \n",
    "    def precit_model_sig_bi(self):\n",
    "        pred = self.model_sig_bi.predict()\n",
    "        return np.argmax(pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"MLP\n",
    "\tdata_manipulate(self)\n",
    "\tbuild_model_all(self)\n",
    "\tbuild_learn_model(self)\n",
    "\tbuild_learn_model_sig(self)\n",
    "\tbuild_learn_model_bi(self)\n",
    "\tbuild_learn_model_sig_bi(self)\n",
    "\tpredict_all(self)\n",
    "\tpredict_model(self)\n",
    "\tpredict_model_sig(self)\n",
    "\tpredict_model_bi(self)\n",
    "\tpredict_model_sig_bi(self)\n",
    "\"\"\"\n",
    "obj1 = MLP()\n",
    "obj1.data_manipulate()\n",
    "obj1.build_learn_all()\n",
    "#result = obj1.predict_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def destroy():\n",
    "    cnx.close()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    init_mysql()\n",
    "    \n",
    "    joongang_crawling()\n",
    "    joongang_cul()\n",
    "    chosun_crawling()\n",
    "    chosun_biz()\n",
    "    donga_crawling()\n",
    "    \n",
    "    ret = cal_weight()\n",
    "    \n",
    "    \n",
    "    destroy()\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    startTime = time.time()\n",
    "    tfidf_raw = main()\n",
    "    endTime = time.time()\n",
    "    print(endTime - startTime)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
