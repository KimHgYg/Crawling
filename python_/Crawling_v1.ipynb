{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import sshtunnel\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "def init_mysql():\n",
    "    global cnx\n",
    "    cnx = mysql.connector.connect(user='user1', password='tkfkdgody1!', host='52.79.185.101', database='Crawling')\n",
    "    global cursor\n",
    "    cursor = cnx.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_foreground = (\"INSERT IGNORE INTO foreground (title, press, link, day, class) \"\n",
    "                  \"VALUES (%(title)s, %(press)s, %(link)s, %(day)s, %(class)s)\")\n",
    "add_background = (\"INSERT IGNORE INTO background (tfidf,link) \"\n",
    "                 \"VALUES (%(tfidf)s, %(link)s)\")\n",
    "clear_db = (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "def get_html(url):\n",
    "    _html = \"\"\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding='utf-8'\n",
    "    if resp.status_code == 200 :\n",
    "        _html = resp.text\n",
    "    return _html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정치 경제 스포츠 연예 사회 국제\n",
    "#politics economy sports enter national inter\n",
    "#조선일보 경제는 biz.chosun.com...\n",
    "CHOSUN = [(\"http://news.chosun.com/politics\",'politics'),(\"http://news.chosun.com/sports\",'sports'), (\"http://news.chosun.com/ent\",'enter'), (\"http://news.chosun.com/national\",'national'),( \"http://news.chosun.com/international\",'inter')]\n",
    "\n",
    "#동아일보는 패턴 통일\n",
    "DONGA = [(\"http://news.donga.com/Politics\",'politics'),(\"http://news.donga.com/Economy\",'economy'),(\"http://news.donga.com/Sports\",'sports'),(\"http://news.donga.com/Enter\",'enter'),(\"http://news.donga.com/Society\",'national'),(\"http://news.donga.com/Inter\",'inter')]\n",
    "\n",
    "#중앙일보 연예가 가요, 방송, 영화로 나눠져있음\n",
    "JOONGANG = [(\"http://news.joins.com/politics\",'politics'),(\"http://news.joins.com/money\",'economy'),(\"http://news.joins.com/sports\",'sports'),(\"http://news.joins.com/society\",'national'),(\"http://news.joins.com/world\",'inter')]\n",
    "JOONGANG_CUL = [(\"http://news.joins.com/culture/song/list\",'enter'),(\"http://news.joins.com/culture/broadcast/list\",'enter'),(\"http://news.joins.com/culture/movie/list\",'enter')]\n",
    "\n",
    "#기사 내용을 담을 변수\n",
    "\n",
    "num_press = 3\n",
    "num_class = 6\n",
    "docs = [[] for i in range(num_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    중앙일보 날짜 가져오기 힘들어서 datetime 패키지를 이용하여 임의로 날짜 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chosun_crawling():\n",
    "    data_foreground = []\n",
    "    i = 0\n",
    "    for clas in CHOSUN:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        for lt in soup.find_all(\"dt\") :\n",
    "            try :\n",
    "                tmp = lt.find('a')\n",
    "                link = tmp.get('href')\n",
    "                day = link.split('/')\n",
    "                day = day[6]+day[7]+day[8]\n",
    "                text = tmp.get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day, 'class' : clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class' : 'par'}).get_text(),link))\n",
    "            except : \n",
    "                print('error!!!',link)\n",
    "        #docs[i].append(tmp_list)\n",
    "        i = i + 1\n",
    "        if i == 1 :\n",
    "            i = i + 1\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return\n",
    "\n",
    "def chosun_biz():\n",
    "    html = get_html('http://biz.chosun.com')\n",
    "    print('http://biz.chosun.com')\n",
    "    soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "    data_foreground = []\n",
    "    head = soup.find(\"div\", attrs={'class' : 'mt_art_tit'})\n",
    "    tmp = head.find('a')\n",
    "    link = tmp.get('href')\n",
    "    day = link.split('/')\n",
    "    day = day[6]+day[7]+day[8]\n",
    "    text = tmp.get_text()\n",
    "    data_foreground.append({'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day, 'class' : 'economy'})\n",
    "    \n",
    "    html = get_html(link)\n",
    "    soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "    docs[1].append((soup2.find('div',id = 'article_2011', attrs={'class':'article'}).get_text(),link))\n",
    "    \n",
    "    art = soup.find(\"div\", attrs={'class' : 'mc_art_lst'})\n",
    "    for lt in art.find_all(\"li\", limit = 30) :\n",
    "        try :\n",
    "            tmp = lt.find('a')\n",
    "            #print(tmp)\n",
    "            link = tmp.get('href')\n",
    "            day = link.split('/')\n",
    "            day = day[6]+day[7]+day[8]\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'chosun',  'link' : link, 'day' : day,'class':'economy'})\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[1].append((soup2.find('div',id = 'article_2011', attrs={'class':'article'}).get_text(),link))\n",
    "        except : \n",
    "            print('error!!!',link)\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donga_crawling():\n",
    "    data_foreground = []\n",
    "    #docs = []\n",
    "    i = 0\n",
    "    for clas in DONGA:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        try:\n",
    "            tmp = soup.find('div',attrs={'class':'articleTop'})\n",
    "            head = tmp.find(\"div\", attrs={'class' : 'articleMain'})\n",
    "            #해드라인_main\n",
    "            head_main = head.find('a')\n",
    "            link = head_main.get('href')\n",
    "            day = link.split('/')[6]\n",
    "            text = head.find(attrs={'class' : 'title'}).get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            #print(data_foreground)\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "            for head_sub in tmp.find_all('li') :\n",
    "                art = head_sub.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "\n",
    "        except :\n",
    "            for head in tmp.find_all('div',attrs={'class':'artivleMain'}):\n",
    "                art = head.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "\n",
    "                \n",
    "            for head in tmp.find_all('div',attrs={'class':'artivleMain02'}):\n",
    "                art = head.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.find(attrs={'class':'title'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "\n",
    "        try:\n",
    "            #issue\n",
    "            content_issue = soup.find('div',attrs={'class':'issueList'})\n",
    "            issue = content_issue.find('a', attrs={'class' : 'tit'})\n",
    "            link = issue.get('href')\n",
    "            day = link.split('/')[6]\n",
    "            text = issue.get_text();\n",
    "            data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html = get_html(link)\n",
    "            soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div',attrs={'class':'atrticle_txt'}).get_text(),link))\n",
    "            for issue_sub in content_issue.find_all('li') :\n",
    "                art = issue_sub.find('a')\n",
    "                link = art.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = art.get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "        except :\n",
    "            print('error!!!',link)\n",
    "            \n",
    "        try:\n",
    "            #최신기사\n",
    "            contents = soup.find('div',attrs={'class':'articleList_con'})\n",
    "            for art in contents.find_all('div',attrs={'class':'rightList'}) :\n",
    "                tmp = art.find('a')\n",
    "                link = tmp.get('href')\n",
    "                day = link.split('/')[6]\n",
    "                text = tmp.find(attrs={'class':'tit'}).get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'donga',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html = get_html(link)\n",
    "                soup2 = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div',attrs={'class':'article_txt'}).get_text(),link))\n",
    "        except :\n",
    "            print('error!!!',link)\n",
    "        i = i + 1\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joongang_crawling():\n",
    "    day = datetime.today().strftime(\"%Y%m%d\")\n",
    "    data_foreground = []\n",
    "    i = 0\n",
    "    for clas in JOONGANG:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        origin = soup.find('div',id='content')\n",
    "        #해드라인\n",
    "        try:\n",
    "            art = origin.find('dt')\n",
    "            tmp = art.find('a')\n",
    "            link = tmp.get('href')\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html2 = get_html(link)\n",
    "            soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "            docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "        except :\n",
    "            #sport 해드라인\n",
    "            try:\n",
    "                for art in origin.find_all('div',attrs={'class':'slide'}):\n",
    "                    tmp = art.find(attrs={'class':'headline mg'})\n",
    "                    tmp2 = tmp.find('a')\n",
    "                    link = tmp2.get('href')\n",
    "                    text = tmp2.get_text();\n",
    "                    data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                    html2 = get_html(link)\n",
    "                    soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                    docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "            except:\n",
    "                print('error!!!',link)\n",
    "        \n",
    "        #실시간 주요뉴스\n",
    "        try:\n",
    "            art = origin.find('div',attrs={'class':'default_realtime'})\n",
    "            art = art.find('ul',id='ulItems')\n",
    "            #print(art)\n",
    "            for content in art.find_all('li'):\n",
    "                #print(content)\n",
    "                tmp = content.find(attrs={'class':'headline mg'})\n",
    "                tmp2 = tmp.find('a')\n",
    "                link = 'http://news.joins.com/' + tmp2.get('href')\n",
    "                text = tmp2.get_text()\n",
    "                data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                html2 = get_html(link)\n",
    "                soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "        except:\n",
    "            try:\n",
    "                art = origin.find('div',attrs={'class':'combination_today'})\n",
    "                art = art.find('div',attrs={'class':'bd'})\n",
    "                for content in art.find_all('li'):\n",
    "                    tmp = content.find(attrs={'class':'headline mg'})\n",
    "                    tmp2 = tmp.find('a')\n",
    "                    link = 'http://news.joins.com/' + tmp2.get('href')\n",
    "                    text = tmp2.get_text()\n",
    "                    data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "                    html2 = get_html(link)\n",
    "                    soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "                    docs[i].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "            except :\n",
    "                print('error!!!',link)\n",
    "        i = i + 1\n",
    "        if i == 3 :\n",
    "            i = i + 1\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return \n",
    "\n",
    "def joongang_cul():\n",
    "    day = datetime.today().strftime(\"%Y%m%d\")\n",
    "    dosc = []\n",
    "    #tmp_list = []\n",
    "    data_foreground = []\n",
    "    for clas in JOONGANG_CUL:\n",
    "        html = get_html(clas[0])\n",
    "        print(clas[0])\n",
    "        soup = BeautifulSoup(html, 'lxml', from_encoding='utf-8')\n",
    "        \n",
    "        origin = soup.find('div',id='content')\n",
    "        for content in origin.find_all('li'):\n",
    "            art = content.find(attrs={'class':'headline mg'})\n",
    "            tmp = art.find('a')\n",
    "            link = 'http://news.joins.com' + tmp.get('href')\n",
    "            text = tmp.get_text()\n",
    "            data_foreground.append({'title' : text, 'press' : 'joongang',  'link' : link, 'day' : day,'class':clas[1]})\n",
    "            html2 = get_html(link)\n",
    "            soup2 = BeautifulSoup(html2, 'lxml', from_encoding='utf-8')\n",
    "            docs[3].append((soup2.find('div', attrs={'class':'article_body'}, id = 'article_body').get_text(),link))\n",
    "    cursor.executemany(add_foreground, data_foreground)\n",
    "    cnx.commit()\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    scikit-Learn 패키지를 이용한 TF-IDF 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,  operator, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Twitter, Kkma\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "stopwords = ['.',',','\\n','\\xa0',re.compile('^A-Za-z*$')]\n",
    "\n",
    "\n",
    "def cal_weight():\n",
    "    tfidf_list = []\n",
    "    tfidf_list_raw = []\n",
    "    t = Twitter()\n",
    "    k = Kkma()\n",
    "    for i in range(num_class):\n",
    "    #for i in range(1):\n",
    "        nouns = []\n",
    "        for article in docs[i]:\n",
    "            if article[0] is not '':\n",
    "                nouns.append(' '.join([noun for noun in t.nouns(str(article[0]))]))        \n",
    "        vec = TfidfVectorizer(stop_words = stopwords)\n",
    "        fitted = vec.fit(nouns)\n",
    "        tfidf_res = fitted.transform(nouns)\n",
    "        vocab = fitted.get_feature_names()\n",
    "        j = 0\n",
    "        for article in tfidf_res.toarray():\n",
    "            idf = sorted(zip(vocab,article), key=lambda kv:kv[1])[-3:]\n",
    "            tmp = idf[0][0]+' '+idf[1][0] + ' ' + idf[2][0]\n",
    "            tfidf_list.append({'tfidf':tmp,'link':docs[i][j][1]})\n",
    "            j = j + 1\n",
    "    cursor.executemany(add_background, tfidf_list)\n",
    "    cnx.commit()\n",
    "    cursor.execute('delete from foreground where link not in (select link from background);')\n",
    "    cnx.commit()\n",
    "    print('cal_weight done!')\n",
    "    return tfidf_list_raw"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    DB에 있는 사용자 기록을 학습하는 학습 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mysql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional 모델 만들기!\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import *\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "def build_model(categori_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(SimpleRNN(128, return_sequences=True), input_shape=(10,1)))\n",
    "    model.add(Bidirectional(SimpleRNN(128, return_sequences=True)))\n",
    "    model.add(Dense(1385, activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    }
   ],
   "source": [
    "def recommand():\n",
    "    global cursor\n",
    "    \n",
    "    cursor.execute('select * from user_tfidf')\n",
    "    user_tfidf_info = cursor.fetchall()\n",
    "    cursor.execute('select * from user_pri')\n",
    "    user_pri_info = cursor.fetchall()\n",
    "    \n",
    "    token = Tokenizer()\n",
    "    \n",
    "    \n",
    "    \n",
    "    user_tfidf = pd.DataFrame(user_tfidf_info,columns = ['ID','tfidf'])\n",
    "    user_pri = pd.DataFrame(user_pri_info,columns = ['ID','age','gender','inter1','inter2','inter3'])\n",
    "    \n",
    "    user_tfidf_value = user_tfidf.drop(columns = ['ID']).values\n",
    "    user_tfidf = user_tfidf.drop(['tfidf'],axis=1)\n",
    "    \n",
    "    i = 0\n",
    "    tmp = []\n",
    "    for arg in user_tfidf_value:\n",
    "        tmp.append(' '.join(arg).split(' '))\n",
    "    \n",
    "    token.fit_on_texts(tmp)\n",
    "    \n",
    "    tmp = token.texts_to_sequences(tmp)\n",
    "    \n",
    "    tmp = pd.DataFrame(tmp,columns = ['tfidf','tfidf2','tfidf3'])\n",
    "    \n",
    "    user_tfidf = pd.concat([user_tfidf,tmp],axis=1)\n",
    "    \n",
    "    merged = pd.merge(user_tfidf,user_pri)\n",
    "    \n",
    "    ID = user_tfidf.ID.unique()\n",
    "    \n",
    "    i = 0\n",
    "    train_X = merged.loc[:,'tfidf':].values\n",
    "    train_X.reshape(-1,8,1)\n",
    "    train_Y = []\n",
    "    \n",
    "    for id in ID:\n",
    "        user_private = merged.loc[merged['ID'] == id, 'tfidf' : 'tfidf3' ].values\n",
    "        user_private = np.vstack((user_private[1:],user_private[0]))\n",
    "        train_Y.extend(user_private)\n",
    "        \n",
    "    train_Y = to_categorical(np.array(train_Y).reshape(-1,3,1))\n",
    "    print(train_Y.shape[2])\n",
    "recommand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def destroy():\n",
    "    cursor.close()\n",
    "    cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    init_mysql()\n",
    "    \n",
    "    joongang_crawling()\n",
    "    joongang_cul()\n",
    "    chosun_crawling()\n",
    "    chosun_biz()\n",
    "    donga_crawling()\n",
    "    \n",
    "    ret = cal_weight()\n",
    "    \n",
    "    #destroy()\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.joins.com/politics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:146: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.joins.com/money\n",
      "http://news.joins.com/sports\n",
      "http://news.joins.com/society\n",
      "error!!! http://news.joins.com//article/23047215\n",
      "http://news.joins.com/world\n",
      "http://news.joins.com/culture/song/list\n",
      "http://news.joins.com/culture/broadcast/list\n",
      "http://news.joins.com/culture/movie/list\n",
      "http://news.chosun.com/politics\n",
      "error!!! http://forum.chosun.com/message/messageView.forum?bbs_id=1010&message_id=1382936\n",
      "error!!! http://forum.chosun.com/message/messageView.forum?bbs_id=1010&message_id=1382930\n",
      "http://news.chosun.com/sports\n",
      "http://news.chosun.com/ent\n",
      "http://news.chosun.com/national\n",
      "error!!! http://news.chosun.com/site/data/html_dir/2018/10/16/2018101600401.html\n",
      "error!!! http://news.chosun.com/site/data/html_dir/2018/10/16/2018101600401.html\n",
      "error!!! http://news.chosun.com/site/data/html_dir/2018/10/16/2018101600401.html\n",
      "error!!! http://news.chosun.com/site/data/html_dir/2018/10/16/2018101600401.html\n",
      "error!!! http://forum.chosun.com/message/messageView.forum?bbs_id=1040&message_id=1382986\n",
      "error!!! http://forum.chosun.com/message/messageView.forum?bbs_id=1030&message_id=1382987\n",
      "http://news.chosun.com/international\n",
      "http://biz.chosun.com\n",
      "http://news.donga.com/Politics\n",
      "error!!! http://news.donga.com/Politics/3/00030300000001/20181016/92414877/1\n",
      "http://news.donga.com/Economy\n",
      "error!!! http://news.donga.com/Economy/3/70010000000919/20181016/92411274/1\n",
      "http://news.donga.com/Sports\n",
      "error!!! http://news.donga.com/Top/3/all/20181016/92422089/1\n",
      "http://news.donga.com/Enter\n",
      "error!!! http://news.donga.com/Top/3/all/20181016/92415767/2\n",
      "http://news.donga.com/Society\n",
      "error!!! http://news.donga.com/Society/3/030125/20181012/92362808/1\n",
      "http://news.donga.com/Inter\n",
      "error!!! http://news.donga.com/Inter/3/021605/20181016/92417090/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal_weight done!\n",
      "215.955726146698\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    startTime = time.time()\n",
    "    tfidf_raw = main()\n",
    "    endTime = time.time()\n",
    "    print(endTime - startTime)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
